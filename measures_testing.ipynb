{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Measures of Performance\n",
    "\n",
    "This notebook is intended to provide a brief introduction to statistical measures of performance (accuracy, precision, recall (sensitivity) and specificity).\n",
    "\n",
    "These four statistical Measures are key to get a good summary of your results as they give insights based on four indicators:<br>\n",
    "- True Positive (TP): Your result says \"True\" and your reference says \"True\".\n",
    "- True Negative (TN): Your result says \"False\" and your reference says \"False\".\n",
    "- False Positive (FP): Your result says \"True\" and your reference says \"False\".\n",
    "- False Negative (FN): Your result says \"False\" and your reference says \"True\".\n",
    "\n",
    "It's important to note these statistical measures are for binary classification (as the example above - True/False). If you want to use them in a multiclass problem it is possible but you have to take one class and leave the rest of the classes as if they were in one group (further explanation and example shown later in the text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity / recall\n",
    "\n",
    "### Definition\n",
    "It is the ratio of how much was classified as Positive and how much must have been Positive.<br>\n",
    "Sensitivity let you know the ratio of positives, it is specially useful in applications in which a True is crucial and prefered over a False result. As an example, in safety functions for autonomous driving if you need to alert in certain circumstances it is prefered to get an incorrect alert that produces safety instead of not having it and needing it. High sensitivity is crucial for safety functions.\n",
    "\n",
    "\n",
    "### Formula / Procedure to find it\n",
    "\n",
    "The equation to compute the sensitivity is as follows: $Sensitivity = \\frac{TP}{TP + FN}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [5,  2,  6, 10,  9,  3,  1,  9,  6,  2,  1,  4,  6,  5,  0,  4,  6,\n",
    "            5,  4,  6,  3,  0,  2,  8,  6,  7,  8,  0,  4,  7,  0,  1,  0,  8,\n",
    "            2,  0, 10,  2,  6,  6,  0,  5,  2,  0, 10,  3,  9,  8,  4,  7,  6,\n",
    "            1, 10,  7, 10,  3,  0,  6,  5,  8,  4,  3,  7,  3,  1,  5,  3,  0,\n",
    "            3,  3,  1,  2,  1,  5,  0,  5,  8,  1, 10,  7,  8,  6,  9,  3,  3,\n",
    "            7,  3,  4, 10,  8,  2,  0,  0,  2,  9,  0,  5,  5,  5,  6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we what an alert every time we get a datapoint equal to 5 or a greater value.<br>\n",
    "For sensitivity comparisson we create two filters (one incorrect as it doesn't take into account the value 5 only greater numbers and one correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(actual, predicted, verbosity=True):\n",
    "    true_positives = np.sum([a is p for a,p in zip(actual,predicted) if a])\n",
    "    false_negatives = np.sum([a is not p for a,p in zip(actual,predicted) if a])\n",
    "    sens = true_positives/(true_positives+false_negatives)\n",
    "    if verbosity:\n",
    "        print(f\"True positives: {true_positives}\\nFalse negatives: {false_negatives}\\nSensitivity: {sens}\\n\")\n",
    "    return sens\n",
    "\n",
    "def alert_trigger_non_inclusive(data):\n",
    "    return np.array([d>5 for d in data])\n",
    "\n",
    "def alert_trigger_inclusive(data):\n",
    "    return np.array([d>=5 for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = alert_trigger_inclusive(data_set)\n",
    "pred = alert_trigger_non_inclusive(data_set)\n",
    "sensitivity(actual, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a sensitivity of 78%\n",
    "It was really bad only by taking out one limit testing condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alert_trigger_inclusive_threshold(data, thresh):\n",
    "    return np.array([d>=thresh for d in data])\n",
    "\n",
    "def threshold(min_t, max_t, data, func):\n",
    "    df = pd.DataFrame()\n",
    "    measure = list()\n",
    "    current_thresh = list()\n",
    "    actual = alert_trigger_inclusive(data)\n",
    "    for t in range(min_t,max_t):\n",
    "        pred = alert_trigger_inclusive_threshold(data, t)\n",
    "        measure.append(func(actual, pred, verbosity=False))\n",
    "        current_thresh.append(t)\n",
    "    df['func'] = measure\n",
    "    df['threshold'] = current_thresh\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sens = threshold(0, 10, data_set, sensitivity)\n",
    "fig = go.Figure(data=go.Scatter(x=df_sens['threshold'],\n",
    "                                y=df_sens[\"func\"],\n",
    "                                text=[f\"Threshold: {_}\" for _ in df_sens[\"threshold\"]],\n",
    "                                mode='markers+lines'))\n",
    "fig.update_layout(title_text=\"Behaviour of Sensitivity with different thresholds\",\n",
    "                  xaxis=dict(title=\"Threshold\"),\n",
    "                  yaxis=dict(title=\"Sensitivity\")\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to take sensitivity into account if your application is critical and needs True results (prefered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "\n",
    "### Definition\n",
    "Specificity is the opposite of sensitivity. It is the ratio of correct classification as negative and all the samples that are indeed negative.\n",
    "\n",
    "### Formula / Procedure to find it\n",
    "\n",
    "The equation to compute the specificity is as follows: $Specificity = \\frac{TN}{FP + TN}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same dataset (previously created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(actual, predicted, verbosity=True):\n",
    "    true_negatives = np.sum([a is p for a,p in zip(actual,predicted) if not a])\n",
    "    false_positives = np.sum([a is not p for a,p in zip(actual,predicted) if not a])\n",
    "    spec = true_negatives/(true_negatives+false_positives)\n",
    "    if verbosity:\n",
    "        print(f\"True negatives: {true_negatives}\\nFalse positives: {false_positives}\\nSpecificity: {spec}\\n\")\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = alert_trigger_inclusive(data_set)\n",
    "pred = alert_trigger_non_inclusive(data_set)\n",
    "specificity(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec = threshold(0, 10, data_set, specificity)\n",
    "fig = go.Figure(data=go.Scatter(x=df_spec['threshold'],\n",
    "                                y=df_spec[\"func\"],\n",
    "                                text=[f\"Threshold: {_}\" for _ in df_sens[\"threshold\"]],\n",
    "                                mode='markers+lines'))\n",
    "fig.update_layout(title_text=\"Behaviour of Specificity with different thresholds\",\n",
    "                  xaxis=dict(title=\"Threshold\"),\n",
    "                  yaxis=dict(title=\"Specificity\")\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this classification gets a better score classifing negatives correctly that positives. Getting both is desired and we need a balance between sensitiviy and specificity to get a good classifier but depending on the application you can select the metric needed.<br>\n",
    "We can get a really useful visualization having these two metrics (Sensitivity and Specificity): The ROC curve (Receiver Operating Characteristic)<br>\n",
    "But we have to modify our classifier to receive thresholds in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alert_trigger_inclusive_threshold(data, thresh):\n",
    "    return np.array([d>=thresh for d in data])\n",
    "\n",
    "def threshold_roc(min_t, max_t, data):\n",
    "    df = pd.DataFrame()\n",
    "    sen_t = list()\n",
    "    spe_t = list()\n",
    "    current_thresh = list()\n",
    "    actual = alert_trigger_inclusive(data)\n",
    "    for t in range(min_t,max_t):\n",
    "        pred = alert_trigger_inclusive_threshold(data, t)\n",
    "        sen_t.append(sensitivity(actual, pred, verbosity=False))\n",
    "        spe_t.append(specificity(actual, pred, verbosity=False))\n",
    "        current_thresh.append(t)\n",
    "    df['sensitivity'] = sen_t\n",
    "    df['specificity'] = spe_t\n",
    "    df['threshold'] = current_thresh\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = threshold_roc(0, 11, data_set)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=1-df['specificity'],\n",
    "                                y=df[\"sensitivity\"],\n",
    "                                text=[f\"Threshold: {_}\" for _ in df[\"threshold\"]],\n",
    "                                mode='markers+lines'))\n",
    "fig.update_layout(title_text=\"ROC for integers threshold classification [0,10]\",\n",
    "                  xaxis=dict(title=\"1 - Specificity\"),\n",
    "                  yaxis=dict(title=\"Sensitivity\")\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see en the plot, while we get closer to the threshold of 5 (the one we know is the right one) we tend to get better results for sensitivity and specificity, reaching the maximum score at threshold 5 with a perfect score of both measures.<br>\n",
    "In this case we got a perfect score, in real applications we might determine how good our classifier is by determining the best combination of sensitivity and specificity if we don't get a perfect score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to take specificity into account if your application is critical and needs False results (prefered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "### Definition\n",
    "The precision can be seen as the percentage of real True scenarios given the total amounts the classifier considers as True. In other words we can determine in what percentage we can trust the classifier when it gives a True statement.<br>\n",
    "It is the ratio of True Positives and the sum of True Positives and False Positives.\n",
    "\n",
    "### Formula / Procedure to find it\n",
    "\n",
    "The equation to compute the precision is as follows: $Precision = \\frac{TP}{TP + FP}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actual, predicted, verbosity=True):\n",
    "    true_positives = np.sum([a is p for a,p in zip(actual,predicted) if a])\n",
    "    false_positives = np.sum([a is not p for a,p in zip(actual,predicted) if not a])\n",
    "    prec = true_positives/(true_positives+false_positives)\n",
    "    if verbosity:\n",
    "        print(f\"True positives: {true_positives}\\nFalse positives: {false_positives}\\nPrecision: {prec}\\n\")\n",
    "    return prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = alert_trigger_inclusive(data_set)\n",
    "pred = alert_trigger_non_inclusive(data_set)\n",
    "precision(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec = threshold(0, 10, data_set, precision)\n",
    "fig = go.Figure(data=go.Scatter(x=df_prec['threshold'],\n",
    "                                y=df_prec[\"func\"],\n",
    "                                text=[f\"Threshold: {_}\" for _ in df_sens[\"threshold\"]],\n",
    "                                mode='markers+lines'))\n",
    "fig.update_layout(title_text=\"Behaviour of Precision with different thresholds\",\n",
    "                  xaxis=dict(title=\"Threshold\"),\n",
    "                  yaxis=dict(title=\"Precision\")\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get precision to know how much you can trust in a True result from your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "### Definition\n",
    "The accuracy gives an overall hint of how good the classification is performed, you have to take all the correct classifications (Trues or Falses) and divide by the number of samples.\n",
    "\n",
    "### Formula / Procedure to find it\n",
    "\n",
    "The equation to compute the accuracy is as follows: $Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual, predicted, verbosity=True):\n",
    "    true_positives = np.sum([a is p for a,p in zip(actual,predicted) if a])\n",
    "    true_negatives = np.sum([a is p for a,p in zip(actual,predicted) if not a])\n",
    "    false_positives = np.sum([a is not p for a,p in zip(actual,predicted) if not a])\n",
    "    false_negatives = np.sum([a is not p for a,p in zip(actual,predicted) if a])\n",
    "    acc = (true_positives + true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
    "    if verbosity:\n",
    "        print(f\"True positives: {true_positives}\\nTrue negatives: {true_negatives}\\n\",\n",
    "              f\"False positives: {false_positives}\\nFalse negatives: {false_negatives}\\nAccuracy: {acc}\\n\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = alert_trigger_inclusive(data_set)\n",
    "pred = alert_trigger_non_inclusive(data_set)\n",
    "accuracy(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = threshold(0, 10, data_set, accuracy)\n",
    "fig = go.Figure(data=go.Scatter(x=df_acc['threshold'],\n",
    "                                y=df_acc[\"func\"],\n",
    "                                text=[f\"Threshold: {_}\" for _ in df_sens[\"threshold\"]],\n",
    "                                mode='markers+lines'))\n",
    "fig.update_layout(title_text=\"Behaviour of Accuracy with different thresholds\",\n",
    "                  xaxis=dict(title=\"Threshold\"),\n",
    "                  yaxis=dict(title=\"Accuracy\")\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that Accuracy is not like the other metrics, you get a peak in the right threshold and the it goes down again.<br>\n",
    "In conclusion, with accuracy you can get a better idea of the result. If want more details you can use sensitivity and specificity to get the ROC curve and also determine the best threshold that suits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "You can test your learning of the introduction of central tendency measures next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def __init__(self):\n",
    "        self.questions = list()\n",
    "        self.answers = list()\n",
    "        self.correct_answers = 0\n",
    "        self.score = 0\n",
    "\n",
    "    def add_element(self, q, a):\n",
    "        self.questions.append(q)\n",
    "        self.answers.append(a)\n",
    "\n",
    "    def remove_element(self, index):\n",
    "        self.questions.pop(index)\n",
    "        self.answers.pop(index)\n",
    "        \n",
    "    def show_answer(self, index):\n",
    "        print(f\"Q{index}: {self.questions[index-1]} - Ans_{index}: {self.answers[index-1]}\")\n",
    "    \n",
    "    def show_answers(self):\n",
    "        for index, (q, a) in enumerate(zip(self.questions, self.answers)):\n",
    "            print(f\"Q{index+1}: {q} - Ans_{index+1}: {a}\")\n",
    "    \n",
    "    def build_from_csv(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        for index in range(df.shape[0]):\n",
    "            self.add_element(df['Questions'][index], df['Answers'][index])\n",
    "    \n",
    "    def visualize_score(self):\n",
    "        fig = go.Figure(data=[go.Pie(labels=[\"Correct\", \"Incorrect\"],\n",
    "                                     values=[self.score, 100-self.score],\n",
    "                                     marker_colors=['rgb(10,100,10)', 'rgb(230,70,70)'],\n",
    "                                     hole=.3)])\n",
    "        fig.show()\n",
    "\n",
    "    def test(self):\n",
    "        self.correct_answers = 0\n",
    "        for index, (q, a) in enumerate(zip(self.questions, self.answers)):\n",
    "            current_answer = ''\n",
    "            while len(str(current_answer))==0:\n",
    "                current_answer = input(f\"Q{index+1}: \" + q)\n",
    "                if len(current_answer)>0:\n",
    "                    current_answer = np.round(float(current_answer),2)\n",
    "                    self.correct_answers += int(current_answer == a)\n",
    "                    if a==current_answer:\n",
    "                        print(\"Correct\")\n",
    "                    else:\n",
    "                        print(\"Incorrect\")\n",
    "        self.score =  100*np.sum(self.correct_answers)/len(self.questions)\n",
    "        \n",
    "        print(f\"Your score: {self.score}\")\n",
    "        self.visualize_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam = test()\n",
    "exam.build_from_csv(\"https://raw.githubusercontent.com/Ricardo-DG/data_analytics_training/main/measures_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to see the answers uncomment and run the following line\n",
    "\n",
    "# exam.show_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to see a specific answer uncomment and run the following line\n",
    "# (make sure to replace \"index\" with the number of the question you want to know the answer).\n",
    "\n",
    "# exam.show_answer(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = exam.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
